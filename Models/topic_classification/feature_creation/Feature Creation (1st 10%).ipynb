{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5574b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd12ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_text = pd.read_csv('GNI88.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0880a9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article ID</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Content</th>\n",
       "      <th>Media Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Published Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3777409</td>\n",
       "      <td>Modernized Tu-160 to boost Russia's long-range...</td>\n",
       "      <td>Modernized Tu-160 to boost Russia's long-range...</td>\n",
       "      <td>Defense News</td>\n",
       "      <td>Bodner, Matthew</td>\n",
       "      <td>2018-02-15 00:00:00+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3764250</td>\n",
       "      <td>Give Trump more nuclear weapons and more ways ...</td>\n",
       "      <td>Give Trump more nuclear weapons and more ways ...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Collina, Tom Z.</td>\n",
       "      <td>2018-02-02 00:00:00+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3764251</td>\n",
       "      <td>Trump wants to build smaller atomic weapons; R...</td>\n",
       "      <td>Trump wants to build smaller atomic weapons; R...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>Cloud, David</td>\n",
       "      <td>2018-02-03 00:00:00+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764252</td>\n",
       "      <td>Egypt and Israel Secretly Allied In Sinai Battle</td>\n",
       "      <td>Egypt and Israel Secretly Allied In Sinai Batt...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Kirkpatrick, David D.</td>\n",
       "      <td>2018-02-04 00:00:00+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3764253</td>\n",
       "      <td>Listen: Top Armed Services Dem worried about r...</td>\n",
       "      <td>Listen: Top Armed Services Dem worried about r...</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>Simendinger, Alexis</td>\n",
       "      <td>2018-02-03 00:00:00+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article ID                                           Headline  \\\n",
       "0     3777409  Modernized Tu-160 to boost Russia's long-range...   \n",
       "1     3764250  Give Trump more nuclear weapons and more ways ...   \n",
       "2     3764251  Trump wants to build smaller atomic weapons; R...   \n",
       "3     3764252   Egypt and Israel Secretly Allied In Sinai Battle   \n",
       "4     3764253  Listen: Top Armed Services Dem worried about r...   \n",
       "\n",
       "                                             Content          Media Name  \\\n",
       "0  Modernized Tu-160 to boost Russia's long-range...        Defense News   \n",
       "1  Give Trump more nuclear weapons and more ways ...                 CNN   \n",
       "2  Trump wants to build smaller atomic weapons; R...   Los Angeles Times   \n",
       "3  Egypt and Israel Secretly Allied In Sinai Batt...  The New York Times   \n",
       "4  Listen: Top Armed Services Dem worried about r...            The Hill   \n",
       "\n",
       "                  Author          Published Date  \n",
       "0        Bodner, Matthew  2018-02-15 00:00:00+00  \n",
       "1        Collina, Tom Z.  2018-02-02 00:00:00+00  \n",
       "2           Cloud, David  2018-02-03 00:00:00+00  \n",
       "3  Kirkpatrick, David D.  2018-02-04 00:00:00+00  \n",
       "4    Simendinger, Alexis  2018-02-03 00:00:00+00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data = pd.read_json(\"gni88.json\", lines=True)\n",
    "article_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6920a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3682869",
   "metadata": {},
   "source": [
    "# Extract first 10% of article content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00306a1a",
   "metadata": {},
   "source": [
    "make new content column -- does not include title, author, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00a03cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take everything after date in content\n",
    "def justContent(content):\n",
    "    content = ''.join(content.split('\\r\\n\\r\\n')[2:])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc38e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['Content2'] = article_data['Content'].apply(lambda x: justContent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e25ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['First_10%_Content_Split'] = article_data['Content2'].apply(lambda x: x.split()[:int(0.1*len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d3aaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "article_data['First_10%_Content'] = article_data['First_10%_Content_Split'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0eb74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes only columns with content\n",
    "df = article_data.loc[article_data['First_10%_Content_Split'].apply(lambda x: len(x) >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9af32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = quote_text.merge(df[['Article ID', 'Content', 'Author', 'Published Date', 'First_10%_Content', 'First_10%_Content_Split']], on = 'Article ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a8d41",
   "metadata": {},
   "source": [
    "# Countries, people, treaties that could point to messages/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e625278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [SEOUL, -, North, Korea's, representatives, as...\n",
       "1         [SEOUL, -, North, Korea's, representatives, as...\n",
       "2         [SEOUL, -, North, Korea's, representatives, as...\n",
       "3         [SEOUL, -, North, Korea's, representatives, as...\n",
       "4         [SEOUL, -, North, Korea's, representatives, as...\n",
       "                                ...                        \n",
       "301268    [But, was, she, celebrated, for, it?, Not, so,...\n",
       "301269    [But, was, she, celebrated, for, it?, Not, so,...\n",
       "301270    [On, the, one, hand,, this, was, hilarious,, s...\n",
       "301271    [On, the, one, hand,, this, was, hilarious,, s...\n",
       "301272    [On, the, one, hand,, this, was, hilarious,, s...\n",
       "Name: First_10%_Content_Split, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['First_10%_Content_Split']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4e06c",
   "metadata": {},
   "source": [
    "# extract treaties\n",
    "creating a column with treaties mentioned in the 1st 10% content\n",
    "- for each 1st 10% content (split), extract any mentions of treaties (take the capital words around it too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "312372bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d8b11dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the 5 words before the word \"Treaty\"\n",
    "def findTreaty(content):\n",
    "    pattern = r\"((?:\\S+\\s+){0,5}\\bTreaty\\b*(?:\\S+\\b\\s*){0,0})\"\n",
    "    return re.findall(pattern, content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79e3b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies findTreaty to 1st 10% content\n",
    "merged_df['Treaty_Instances'] = merged_df['First_10%_Content'].apply(lambda x: findTreaty(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c54678",
   "metadata": {},
   "source": [
    "Given each row in Treaty_Instances --> starting from Treaty, works backwards and returns everything until reaches non-proper word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da9c181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract everything from first mention of \"the\" to Treaty\n",
    "# treaty_instance is each item in merged_df['Treaty_Instances']\n",
    "def propWords_Treaty(treaty_instance):\n",
    "    if treaty_instance == []:\n",
    "        return None\n",
    "    words = treaty_instance[0].split()\n",
    "    for i in np.arange(len(words)):\n",
    "        if words[i] == 'the':\n",
    "            return ' '.join(words[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "00cd93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signatory of the Nuclear Non-Proliferation Treaty']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Treaty_Instances'].value_counts().index[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3317aa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Nuclear Non-Proliferation Treaty'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propWords_Treaty(merged_df['Treaty_Instances'].value_counts().index[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b06b6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies propWords_Treaty to Treaty_Instances\n",
    "merged_df['Treaties'] = merged_df['Treaty_Instances'].apply(lambda x: propWords_Treaty(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df37a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the Nuclear Nonproliferation Treaty                1507\n",
       "the Nuclear Non-Proliferation Treaty               1365\n",
       "the nuclear Non-Proliferation Treaty                790\n",
       "the North Atlantic Treaty                           693\n",
       "the Non-Proliferation Treaty                        683\n",
       "                                                   ... \n",
       "the Arms Trade Treaty                                 1\n",
       "the 1963 Franco-German ElysÃ©e Treaty                  1\n",
       "the long run.\"The Outer Space Treaty                  1\n",
       "the Russian exercises.The North Atlantic Treaty       1\n",
       "the 1967 U.N. Outer Space Treaty                      1\n",
       "Name: Treaties, Length: 235, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Treaties'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7c7ae6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem: if there are more than one instance of \"the\", it includes it\n",
    "# try going from \"Treaty\" backwards, so it finds the last \"the\" first\n",
    "def propWords_Treaty2(treaty_instance):\n",
    "    if treaty_instance == []:\n",
    "        return None\n",
    "    words = treaty_instance[0].split()\n",
    "    i = len(words)-1\n",
    "    while i > 0:#<= (len(words)-1):\n",
    "        if words[i] == 'the':\n",
    "            return ' '.join(words[i:])\n",
    "        else:\n",
    "            i -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9cea18d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signatory of the Nuclear Non-Proliferation Treaty']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Treaty_Instances'].value_counts().index[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e0123e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Nuclear Non-Proliferation Treaty'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propWords_Treaty2(merged_df['Treaty_Instances'].value_counts().index[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1bb19532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies propWords_Treaty to Treaty_Instances\n",
    "merged_df['Treaties2'] = merged_df['Treaty_Instances'].apply(lambda x: propWords_Treaty2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eedb3916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the Nuclear Nonproliferation Treaty              1571\n",
       "the Nuclear Non-Proliferation Treaty             1365\n",
       "the nuclear Non-Proliferation Treaty              790\n",
       "the Non-Proliferation Treaty                      716\n",
       "the North Atlantic Treaty                         704\n",
       "                                                 ... \n",
       "the organization's founding Washington Treaty       1\n",
       "the Iran Nuclear Treaty                             1\n",
       "the Oslo Treaty                                     1\n",
       "the Arms Trade Treaty                               1\n",
       "the summit, North Atlantic Treaty                   1\n",
       "Name: Treaties2, Length: 163, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Treaties2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc67ba",
   "metadata": {},
   "source": [
    "Still some issues with some of the strings:\n",
    "\n",
    "* \"the organization's founding Washington Treaty\"\n",
    "* \"the summit, North Atlantic Treaty\"\n",
    "\n",
    "not all the treaties start with \"the\"\n",
    "future work: improve Treaty extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eb3d9",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41a362",
   "metadata": {},
   "source": [
    "basic text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b191ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         seoul - north korea's representatives assured ...\n",
       "1         seoul - north korea's representatives assured ...\n",
       "2         seoul - north korea's representatives assured ...\n",
       "3         seoul - north korea's representatives assured ...\n",
       "4         seoul - north korea's representatives assured ...\n",
       "                                ...                        \n",
       "301268    but was she celebrated for it? not so much. - ...\n",
       "301269    but was she celebrated for it? not so much. - ...\n",
       "301270    on the one hand, this was hilarious, since on ...\n",
       "301271    on the one hand, this was hilarious, since on ...\n",
       "301272    on the one hand, this was hilarious, since on ...\n",
       "Name: First_10%_Content, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lowercase = merged_df['First_10%_Content'].str.lower()\n",
    "text_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc2372",
   "metadata": {},
   "source": [
    "remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e15e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(document):\n",
    "    no_punct = ''.join([character for character in document if character not in punctuation])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "deaae8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         seoul  north koreas representatives assured th...\n",
       "1         seoul  north koreas representatives assured th...\n",
       "2         seoul  north koreas representatives assured th...\n",
       "3         seoul  north koreas representatives assured th...\n",
       "4         seoul  north koreas representatives assured th...\n",
       "                                ...                        \n",
       "301268    but was she celebrated for it not so much  mei...\n",
       "301269    but was she celebrated for it not so much  mei...\n",
       "301270    on the one hand this was hilarious since on mo...\n",
       "301271    on the one hand this was hilarious since on mo...\n",
       "301272    on the one hand this was hilarious since on mo...\n",
       "Name: First_10%_Content, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_punct = text_lowercase.apply(remove_punctuation)\n",
    "text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0fa26",
   "metadata": {},
   "source": [
    "remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d62b00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digit(document): \n",
    "    no_digit = ''.join([character for character in document if not character.isdigit()]) \n",
    "    return no_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d0536150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         seoul  north koreas representatives assured th...\n",
       "1         seoul  north koreas representatives assured th...\n",
       "2         seoul  north koreas representatives assured th...\n",
       "3         seoul  north koreas representatives assured th...\n",
       "4         seoul  north koreas representatives assured th...\n",
       "                                ...                        \n",
       "301268    but was she celebrated for it not so much  mei...\n",
       "301269    but was she celebrated for it not so much  mei...\n",
       "301270    on the one hand this was hilarious since on mo...\n",
       "301271    on the one hand this was hilarious since on mo...\n",
       "301272    on the one hand this was hilarious since on mo...\n",
       "Name: First_10%_Content, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_digit = text_no_punct.apply(remove_digit)\n",
    "text_no_digit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d750f0e",
   "metadata": {},
   "source": [
    "TOKENIZATION\n",
    "\n",
    "transform text string into vector of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9761fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/atmika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30943ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [seoul, north, koreas, representatives, assure...\n",
       "1    [seoul, north, koreas, representatives, assure...\n",
       "2    [seoul, north, koreas, representatives, assure...\n",
       "3    [seoul, north, koreas, representatives, assure...\n",
       "4    [seoul, north, koreas, representatives, assure...\n",
       "Name: First_10%_Content, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_tokenized = text_no_digit.apply(word_tokenize)\n",
    "text_tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be058cf2",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "510688db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atmika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f29c63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(document):\n",
    "    words = [word for word in document if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7918f797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [seoul, north, koreas, representatives, assure...\n",
       "1         [seoul, north, koreas, representatives, assure...\n",
       "2         [seoul, north, koreas, representatives, assure...\n",
       "3         [seoul, north, koreas, representatives, assure...\n",
       "4         [seoul, north, koreas, representatives, assure...\n",
       "                                ...                        \n",
       "301268    [celebrated, much, meitner, collaborated, seve...\n",
       "301269    [celebrated, much, meitner, collaborated, seve...\n",
       "301270    [one, hand, hilarious, since, monday, dow, sec...\n",
       "301271    [one, hand, hilarious, since, monday, dow, sec...\n",
       "301272    [one, hand, hilarious, since, monday, dow, sec...\n",
       "Name: First_10%_Content, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_stop = text_tokenized.apply(remove_stopwords)\n",
    "text_no_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5bcac",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "89ea4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def stemmer(document):\n",
    "    stemmed_document = [porter.stem(word) for word in document]\n",
    "    return stemmed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64d9923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [seoul, north, korea, repres, assur, south, ko...\n",
       "1         [seoul, north, korea, repres, assur, south, ko...\n",
       "2         [seoul, north, korea, repres, assur, south, ko...\n",
       "3         [seoul, north, korea, repres, assur, south, ko...\n",
       "4         [seoul, north, korea, repres, assur, south, ko...\n",
       "                                ...                        \n",
       "301268    [celebr, much, meitner, collabor, sever, decad...\n",
       "301269    [celebr, much, meitner, collabor, sever, decad...\n",
       "301270    [one, hand, hilari, sinc, monday, dow, secondw...\n",
       "301271    [one, hand, hilari, sinc, monday, dow, secondw...\n",
       "301272    [one, hand, hilari, sinc, monday, dow, secondw...\n",
       "Name: First_10%_Content, Length: 301273, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_stemmed = text_no_stop.apply(stemmer)\n",
    "text_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383c81b",
   "metadata": {},
   "source": [
    "# Document-Term Matrix\n",
    "calculate frequency of words across news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dec784",
   "metadata": {},
   "source": [
    "detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cef9672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "text_detokenized = text_stemmed.apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f53c3",
   "metadata": {},
   "source": [
    "document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "38770d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<301273x419526 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 77216234 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "sparse_dtm = countvec.fit_transform(text_detokenized)\n",
    "sparse_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cef7222",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7k/q0mnk0h51sj9qcvrz1gwfz7c0000gn/T/ipykernel_8988/781803173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_dtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dtm = pd.DataFrame(sparse_dtm.toarray(), columns=countvec.get_feature_names(), index=merged_df.index)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = dtm.sum().sort_values(ascending=False)\n",
    "print(frequencies[frequencies > 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b12a64",
   "metadata": {},
   "source": [
    "future work: figure out why kernel is dying, find most common word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize frequency of words\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "ax = sns.countplot(frequencies)\n",
    "plt.xticks(np.arange(1, 50, step=5), np.arange(1, 50, step=5))\n",
    "\n",
    "\n",
    "plt.xlabel('terms')\n",
    "plt.ylabel(' ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ada8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
