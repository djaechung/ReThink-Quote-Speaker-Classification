{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speaker_name_cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "90vGVH8J7ooi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAgcrkk47wCG",
        "outputId": "24a56f2b-ba15-452e-db43-4795a91f6c2b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "h7JFJmMB8dQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading a fixed version GNI88.csv (replaced one double quote with a single quote in line 369292) to a Pandas df\n",
        "qtes = pd.read_csv(\"/content/drive/MyDrive/data/GNI88_fixed.csv\")"
      ],
      "metadata": {
        "id": "-_c91xsI7xdL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading gni88.json to a Pandas df\n",
        "arts = pd.read_json(\"/content/drive/MyDrive/data/gni88.json\", lines=True)"
      ],
      "metadata": {
        "id": "Y_dH8ynZ77QT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping entries in qtes where the article id is one that exists in the arts df too\n",
        "qtes = qtes.merge(arts, how='inner')[qtes.columns]"
      ],
      "metadata": {
        "id": "22Gqxlhv79tn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Speaker Names"
      ],
      "metadata": {
        "id": "WyNgA2fh8fvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: take the column of quote source names and transform it so that real names are presented in their simplest form and non-human entities like governments are relabeled \"not_name\" so we can filter them out of name-based classification problems."
      ],
      "metadata": {
        "id": "qxKLclwJ9Rqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E: \"Dr. Oski Bear\" ---> \"Oski Bear\""
      ],
      "metadata": {
        "id": "WNjDcMj_9plu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E: \"Government of North Korea\" ---> \"not_name\""
      ],
      "metadata": {
        "id": "weGf9pUL-Vyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name Cleaning Functions"
      ],
      "metadata": {
        "id": "8R0EGRAU8pSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: Lana Elauria\n",
        "\n",
        "def remove_prefix(text):\n",
        "    for prefix in prefixes:\n",
        "      if text.lower().startswith(prefix):\n",
        "        slicer = len(prefix)\n",
        "        return text[slicer:]\n",
        "    return text\n",
        "\n",
        "def remove_suffix(text):\n",
        "    for suffix in suffixes:\n",
        "      if text.endswith(suffix):\n",
        "        slicer = len(suffix)\n",
        "        return text[:-slicer]\n",
        "    return text\n",
        "\n",
        "def regex_trim(rx_list, column, df, replace_value=\"\"):\n",
        "    '''Takes a list of regex patterns, and joins the patterns with an OR (|) separator. \n",
        "    Searches the specified column/df for the pattern and replaces it with value specified (default value-nothing)'''\n",
        "    df[column] = df[column].replace(to_replace=\"|\".join(rx_list), value=replace_value, regex=True)\n",
        "    return df\n",
        "\n",
        "def remove_accents(txt):\n",
        "    \"\"\"Certain outlets (CTV News) do not use accented characters in person names.\n",
        "       Others (CBC News and Global news), always use accented characters in names.\n",
        "       To help normalize these names and get accurate counts of sources, we replace \n",
        "       accented characters with their regular English equivalents.\n",
        "       Example names that are normalized across different outlets using this method:\n",
        "        * François Legault <-> Francois Legault\n",
        "        * Valérie Plante <-> Valerie Plante\n",
        "        * Jean Chrétien <-> Jean Chretien \n",
        "    \"\"\"\n",
        "    txt = re.sub(\"[àáâãäå]\", 'a', txt)\n",
        "    txt = re.sub(\"[èéêë]\", 'e', txt)\n",
        "    txt = re.sub(\"[ìíîïı]\", 'i', txt)\n",
        "    txt = re.sub(\"[òóôõö]\", 'o', txt)\n",
        "    txt = re.sub(\"[ùúûü]\", 'u', txt)\n",
        "    txt = re.sub(\"[ýÿ]\", 'y', txt)\n",
        "    txt = re.sub(\"ç\", 'c', txt)\n",
        "    txt = re.sub(\"ğ\", 'g', txt)\n",
        "    txt = re.sub(\"ñ\", 'n', txt)\n",
        "    txt = re.sub(\"ş\", 's', txt)\n",
        "\n",
        "    # Capitals\n",
        "    txt = re.sub(\"[ÀÁÂÃÄÅ]\", 'A', txt)\n",
        "    txt = re.sub(\"[ÈÉÊË]\", 'E', txt)\n",
        "    txt = re.sub(\"[ÌÍÎÏİ]\", 'I', txt)\n",
        "    txt = re.sub(\"[ÒÓÔÕÖ]\", 'O', txt)\n",
        "    txt = re.sub(\"[ÙÚÛÜ]\", 'U', txt)\n",
        "    txt = re.sub(\"[ÝŸ]\", 'Y', txt)\n",
        "    txt = re.sub(\"Ç\", 'C', txt)\n",
        "    txt = re.sub(\"Ğ\", 'G', txt)\n",
        "    txt = re.sub(\"Ñ\", 'N', txt)\n",
        "    txt = re.sub(\"Ş\", 'S', txt)\n",
        "    return txt\n",
        "\n",
        "def remove_titles(txt):\n",
        "    \"\"\"Method to clean special titles that appear as prefixes or suffixes to\n",
        "       people's names (common especially in articles from British/European sources).\n",
        "       The words that are marked as titles are chosen such that they can never appear\n",
        "       in any form as a person's name (e.g., \"Mr\", \"MBE\" or \"Headteacher\").\n",
        "    \"\"\"\n",
        "    honorifics = [\"Dr\", \"Sir\", \"Dame\", \"Professor\", \"Prof\", \"Rev\"]\n",
        "    titles = [\"QC\", \"CBE\", \"MBE\", \"BM\", \"MD\", \"DM\", \"BHB\", \"CBC\", \"Rep\", \"Rep.\",\n",
        "              \"Reverend\", \"Recorder\", \"Headteacher\", \"Councillor\", \"Cllr\", \"Father\", \"Fr\",\n",
        "              \"Mother\", \"Grandmother\", \"Grandfather\", \"Creator\", \"U.S. Rep\", \"Senator\", \"Sen\", \"Rabbi\", \"Imam\"] # could add \"Judge\" but that could also be someone's name\n",
        "    extras = [\"et al\", \"www\", \"href\", \"http\", \"https\", \"Ref\"]\n",
        "    banned_words = r'|'.join(honorifics + titles + extras)\n",
        "    # Ensure only whole words are replaced (\\b is word boundary)\n",
        "    pattern = re.compile(r'\\b({})\\b'.format(banned_words)) \n",
        "    txt = pattern.sub('', txt)\n",
        "    txt = re.sub(\"^\\.\",\"\",txt)\n",
        "    return txt.strip()\n",
        "\n",
        "def lnfn_parse(txt):\n",
        "    \"\"\"Converts names with \"Last, First\" pattern to \"First Last\" pattern.\n",
        "       Works with multiple \"Last, First\" names, returns \"First Last, First Last, ...\"\n",
        "    \"\"\"\n",
        "    lnfn_split = txt.split(\", \")\n",
        "    fnln_split = lnfn_split[::-1]\n",
        "    fnln = \", \".join([\" \".join(x) for x in zip(fnln_split[0::2], fnln_split[1::2])])\n",
        "    return fnln"
      ],
      "metadata": {
        "id": "k7ZO_hSg8Hks"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful Regex Patterns"
      ],
      "metadata": {
        "id": "5XZhCMCD8t8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#looks for phone number and optional leading spaces/punctuation\n",
        "phonenum_regex = '((?: |, |; |\\. |\\| )?\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}(?: |, |; |\\. |\\| )?)'\n",
        "#looks for email address and optional leading spaces/punctuation\n",
        "email_regex = \"((?: |, |; |\\. |\\| )?[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+(?: |, |; |\\. |\\| )?)\"\n",
        "#looks for title words (case insensitive) and optional leading spaces/punctuation\n",
        "title_regex = '((?: |, |; |\\. |\\| | - )?(?i)(?:Staff Writers?|Editor\\-in\\-Chief|Managing Editor|Political Editor|Editor\\-at\\-large|Columnist|Correspondent|Opinion contributors?|special.*|Capital Bureau)(?: |, |; |\\. |\\| )?)'\n",
        "#capture -, anything after | \n",
        "symbol_regex = ' -|\\|.*$'\n",
        "#capture firstname.lastinitial pattern at end of AJC bylines, \"; .. is . .\" pattern with bios \n",
        "specialpatterns_regex = \"(?: \\w{4,}\\.\\w$)|(?i); .*(?:\\.$| is.*)\"\n",
        "#capture non-name entries including anything after 'from,' and anything containing 'editorial', 'readers', or 'editors' \n",
        "non_name_regex = \".*(?:staff$|staff ).*|Letters to the Editor|from.*|(?i).*editorial.*|(?i).*editors.*|No by-line,|(?i).*readers.*\"\n",
        "#look for news outlets, case insensitive, including optional leading 'the'/connectors/punctuation\n",
        "#For CNN captures anything that comes after\n",
        "outlet_regex = '(?i)(?:, |; | and | for | ?The )?(?i)(?:CNN.*$|Associated Press|New York Times|Washington Times|USA Today|AJC|Green Bay Press-Gazette|Daily Beast|Nation|Houston Chronicle|Sarasota Herald-Tribune|Augusta Chronicle|Arizona Republic|Texas Tribune|Chicago Tribune)'\n",
        "#capture non-comma connectors ('and', ';and', ';', '\\n', '&')\n",
        "connector_regex = '((?i)(?: ;and | and |; *|\\\\n * | & *))'\n",
        "#capture double comma patterns\n",
        "dbl_comma_regex = ', *,+ *'\n",
        "#capture last name, first name pattern\n",
        "# edited to capture names with punctuation (ie. hyphenated names, or names with middle initial)\n",
        "lnfn_regex = \"(^(?:[\\w\\.\\'-])*, (?:[\\w\\.\\'-])*|(?:[\\w\\.\\'-] [\\w\\.])*$)\"\n",
        "#looks for \"The\" preceded and followed by a space, with optional leading comma\n",
        "the_regex = ',? The .*'\n",
        "#looks for \"The\" preceded and followed by a space, with optional leading comma\n",
        "start_the_regex = '^The .*'"
      ],
      "metadata": {
        "id": "Wpirp4MV8Ppj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rx_patterns = [phonenum_regex,\n",
        "               email_regex,\n",
        "               title_regex, \n",
        "               symbol_regex, \n",
        "               specialpatterns_regex, \n",
        "               outlet_regex, \n",
        "               non_name_regex,\n",
        "               the_regex,\n",
        "               start_the_regex]"
      ],
      "metadata": {
        "id": "HzHuUZms8Rkh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_list = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District \", \"of Columbia\", \n",
        "              \"Delaware\", \"Florida\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \n",
        "              \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \n",
        "              \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \n",
        "              \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virgin Islands\", \"Vermont\", \"Wisconsin\", \n",
        "              \"West Virginia\", \"Wyoming\"]\n",
        "\n",
        "#Patterns of non-name Source Name entries\n",
        "notname_regex = r\"unnamed|editorial|\\bthe\\b|\\bof\\b|opponents|election|vote|liberal |conservatives?| for |documents?|expert|citizens|research|voting|financial|journal|reuters|cnn|bulletin| and |newswire| memo|\\bpoll\\b|spokesperson|[0-9]|\\busa\\b\"\n",
        "org_regex= \"statement|committee|institute|report|groups?|association|university|college|center|coalition|advocate|national|league|associated|american|daily\"\n",
        "govt_regex = \"^gop |federal|u\\.s\\.|supreme court|officials?|administration|department|office|congress|campaign|census|white house|democrat|republican|senate |registrar|secretary|commission|agency|us police|government\"\n",
        "court_regex = \"appeals|circuit|lawyer|attorney|records|\\bcourts?\\b|lawsuit\"\n",
        "long_regex = \"\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+\"\n",
        "\n",
        "#Look for strings that do not contain this\n",
        "short_regex = \"\\S+\\s+\\S+\"\n",
        "\n",
        "# regex for one-letter first names\n",
        "one_letter_regex = \"^(\\w\\.)\\W(\\w+)\"\n",
        "\n",
        "notname_regex_list = [notname_regex, org_regex, govt_regex, court_regex, long_regex]"
      ],
      "metadata": {
        "id": "mzI9SEFM8S2U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_strings = ['Mark', 'By Mark', 'No by-line', 'Opinion by Mark', 'Analysis by Mark']\n",
        "\n",
        "#for test in df.head()['author']:\n",
        "#  print(author_cleaning(test))\n",
        "\n",
        "prefixes = ['letter to the editor by ', 'by ', 'opinion by ', 'analysis by ', 'compiled by ', 'por ']\n",
        "\n",
        "suffixes = [';Editor', ' Florida Times-Union', ' Jacksonville Florida Times-Union', ' Milwaukee Journal Sentinel',\n",
        "            ' Capitol Media Services', ' -- Times Staff Writer', 'Appleton Post-Crescent',  \n",
        "            '; Richmond Times-Dispatch', ' SUN STAFF WRITER', ' News Service Of Florida',\n",
        "            ', Palm Beach Post', '; Editor', '; WPR NEWS', \n",
        "            ' Richmond Times-Dispatch', ' -- Times/Herald Tallahassee Bureau', ', RealClearWire', \n",
        "            '  -- Times Political Editor', '; Austin Bureau', ' Tribune News Service', ' Guest Columnist', \n",
        "            '; LA CROSSE TRIBUNE', ', Omaha World-Herald', ' USA TODAY NETWORK',  \n",
        "            ' InsideSources.com', ' Yuma Sun Editor', ', Capitol Beat News Service', ' South Florida Sun Sentinel',\n",
        "            ' Orlando Sentinel', '; Murphy teaches writing at Virginia Tech', \" Washington Bureau\", '; Contributing Writer', '  -- Times/Herald',  \n",
        "            ' Capitol Beat News Service', ' -- PolitiFact', '; Now News Group', ' Tribune Content Agency', \n",
        "            '; WISCONSIN STATE JOURNAL', '; Washington Bureau Chief', ' The Heritage Foundation',\n",
        "            ', Associated Press; The New York Times contributed.', ', Los Angeles Times', ' Atlanta Journal-Constitution', \n",
        "            ' of Capital News Service', 'Por']"
      ],
      "metadata": {
        "id": "BNT--Rsj8U-h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning names"
      ],
      "metadata": {
        "id": "SGZ_N_VT80m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop \"OLD\" labels from name strings\n",
        "qtes[\"Source Name\"] = qtes[\"Source Name\"].str.split(\" OLD\", expand =True)[0]\n",
        "qtes[\"Source Name\"] = qtes[\"Source Name\"].str.split(r\" \\(OLD\\)\", expand =True)[0]\n",
        "\n",
        "#Remove any names labelled \"Organization\"\n",
        "qtes['cleaned_name'] = np.where(qtes['Source Gender'] == \"Organization\", \"not_name\", qtes['Source Name']) \n",
        "\n",
        "#Fill empty cells\n",
        "qtes[\"cleaned_name\"] = qtes[\"Source Name\"].replace(np.nan, \"none\").apply(remove_prefix).apply(remove_suffix).str.title()\n",
        "\n",
        "# replace \" , \" with \", \" to fix some HuffPost author formats\n",
        "qtes[\"cleaned_name\"] = qtes[\"cleaned_name\"].replace(\" , \", \", \")\n",
        "\n",
        "# removing stray \"Por\" prefixes\n",
        "qtes[\"cleaned_name\"] = qtes[\"cleaned_name\"].replace(\"Por \", \"\")\n",
        "\n",
        "#Remove rx pattern matches\n",
        "qtes = regex_trim(rx_patterns, column=\"cleaned_name\", df=qtes)\n",
        "\n",
        "#find non-comma connectors and convert to comma\n",
        "qtes = regex_trim([connector_regex], \"cleaned_name\", df=qtes, replace_value=\", \")\n",
        "\n",
        "#after comma conversion, check for multiple commas together and convert to single comma\n",
        "qtes = regex_trim([dbl_comma_regex], \"cleaned_name\", df=qtes, replace_value=\", \")\n",
        "\n",
        "#strip trailing commas, and leading and trailing whitespace, then check for trailing commas again\n",
        "qtes['cleaned_name'] = qtes['cleaned_name'].str.rstrip(\",\").str.strip().str.rstrip(\",\")\n",
        "\n",
        "#Format names with last name, first name pattern\n",
        "qtes['cleaned_name'] = np.where(qtes['cleaned_name'].str.match(lnfn_regex), \n",
        "                                  qtes['cleaned_name'].apply(lnfn_parse),\n",
        "                                  qtes['cleaned_name'])\n",
        "\n",
        "#Re-run searches to strip out names starting with 'The'\n",
        "qtes = regex_trim([the_regex, start_the_regex], \"cleaned_name\", df=qtes)\n",
        "\n",
        "#Remove accents and titles\n",
        "qtes['cleaned_name'] = qtes['cleaned_name'].apply(remove_accents).apply(remove_titles)\n",
        "\n",
        "#Remove non-name regex matches\n",
        "qtes['cleaned_name'] = np.where(qtes['cleaned_name'].str.lower().str.contains(\n",
        "    \"|\".join(notname_regex_list), regex=True), \n",
        "    \"not_name\", qtes['cleaned_name'])\n",
        "\n",
        "#Remove state matches\n",
        "qtes['cleaned_name'] = np.where(qtes['cleaned_name'].str.contains(\"|\".join(state_list), regex=True), \n",
        "                                  \"not_name\", qtes['cleaned_name']) \n",
        "\n",
        "#Remove one word names\n",
        "qtes['cleaned_name'] = np.where(qtes['cleaned_name'].str.contains(short_regex, regex=True), \n",
        "                                  qtes['cleaned_name'], \"not_name\")\n",
        "\n",
        "# remove one letter (abbreviated) first names\n",
        "qtes['cleaned_name'] = np.where(qtes['cleaned_name'].str.contains(one_letter_regex, regex=True),\n",
        "                                  \"not_name\", qtes['cleaned_name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQhSdKmS8W84",
        "outputId": "60bc252c-fad8-452d-aebc-7a471cbbd7fe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstration of name-cleaned df"
      ],
      "metadata": {
        "id": "p4BoOqFy877E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Narrowing the quotes dataframe down to just records where the source was an \n",
        "# actual name, not a non_name\n",
        "qtes_just_names = qtes[qtes[\"cleaned_name\"] != \"not_name\"]"
      ],
      "metadata": {
        "id": "2zhBDSuU8aOY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstration of cleaned names\n",
        "list(qtes_just_names[\"cleaned_name\"].unique())[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2FCLNFL9Ny7",
        "outputId": "4972f901-63fe-41e2-b562-6f232121e996"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Moon Jae-In',\n",
              " 'Ri Son Gwon',\n",
              " 'Cho Myoung- Gyon',\n",
              " 'Rob Soofer',\n",
              " 'Paul Selva',\n",
              " 'Lisa Foxen',\n",
              " 'Donald Trump',\n",
              " 'Tulsi Gabbard',\n",
              " 'David Ige',\n",
              " 'Mazie Hirono',\n",
              " 'Atji Pai',\n",
              " 'Nikki Haley',\n",
              " 'Hiroyuku Suenaga',\n",
              " 'Adam Smith',\n",
              " 'Kim Jong Un',\n",
              " 'Mike Pence',\n",
              " 'Baek Tae-Hyun',\n",
              " 'Lu Kang',\n",
              " 'James Mattis',\n",
              " 'Trita Parsi']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bun9A0AS9uyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}